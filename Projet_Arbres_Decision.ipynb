{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45207ec6",
   "metadata": {},
   "source": [
    "# Projet : Arbres de Decision, Extensions et Applications\n",
    "\n",
    "**Annee universitaire 2024-2025**\n",
    "\n",
    "---\n",
    "\n",
    "## Table des matieres\n",
    "1. Partie 1 - Revision des concepts et experiences numeriques\n",
    "2. Partie 2 - Implementation d'un mini-arbre de decision \"from scratch\"\n",
    "3. Partie 3 - Extensions : sur-apprentissage, elagage et forets aleatoires\n",
    "4. Partie 4 - Application metier et interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13502501",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 1 - Revision des concepts et experiences numeriques\n",
    "\n",
    "## 1.1 Rappel theorique\n",
    "\n",
    "### Classification supervisee\n",
    "La classification supervisee est une technique d'apprentissage automatique ou:\n",
    "- On dispose d'un ensemble d'exemples (x, y) avec x les attributs et y la classe\n",
    "- Le but est d'apprendre une fonction f telle que f(x) predit y\n",
    "- L'apprentissage est \"supervise\" car on connait les vraies classes\n",
    "\n",
    "### Arbre de decision\n",
    "Un arbre de decision est une structure hierarchique composee de:\n",
    "- **Noeuds internes**: contiennent un test sur un attribut (ex: age > 30?)\n",
    "- **Branches**: representent les resultats du test\n",
    "- **Feuilles**: contiennent la prediction de classe\n",
    "\n",
    "### Mesures d'impurete\n",
    "L'impurete mesure l'heterogeneite des classes dans un noeud. Soit p la proportion de la classe positive:\n",
    "\n",
    "1. **Indice de Gini**: $Gini(p) = 2p(1-p)$ ou $Gini = 1 - \\sum p_i^2$\n",
    "2. **Entropie**: $H(p) = -p\\log_2(p) - (1-p)\\log_2(1-p)$\n",
    "3. **Erreur de classification**: $E(p) = 1 - \\max(p, 1-p)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba4bf1",
   "metadata": {},
   "source": [
    "## 1.3 Implementation des fonctions d'impurete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56729c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(counts):\n",
    "    \"\"\"Calcule l'indice de Gini pour une distribution de classes.\"\"\"\n",
    "    counts = np.array(counts)\n",
    "    total = np.sum(counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    proportions = counts / total\n",
    "    return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "def entropy(counts):\n",
    "    \"\"\"Calcule l'entropie pour une distribution de classes.\"\"\"\n",
    "    counts = np.array(counts)\n",
    "    total = np.sum(counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    proportions = counts / total\n",
    "    proportions = proportions[proportions > 0]\n",
    "    return -np.sum(proportions * np.log2(proportions))\n",
    "\n",
    "def classification_error(counts):\n",
    "    \"\"\"Calcule l'erreur de classification pour une distribution de classes.\"\"\"\n",
    "    counts = np.array(counts)\n",
    "    total = np.sum(counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    proportions = counts / total\n",
    "    return 1 - np.max(proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bab67",
   "metadata": {},
   "source": [
    "## 1.2 Exemples numeriques d'impurete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition des cas de test\n",
    "test_cases = [\n",
    "    (\"Cas equilibre (10/10)\", [10, 10]),\n",
    "    (\"Cas tres pur (18/2)\", [18, 2]),\n",
    "    (\"Cas 9/1\", [9, 1]),\n",
    "    (\"Cas 5/5\", [5, 5]),\n",
    "    (\"Cas 1/9\", [1, 9]),\n",
    "]\n",
    "\n",
    "print(f\"{'Cas':<25} {'Gini':>10} {'Entropie':>10} {'Err. Class':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = []\n",
    "for name, counts in test_cases:\n",
    "    g = gini(counts)\n",
    "    e = entropy(counts)\n",
    "    c = classification_error(counts)\n",
    "    results.append((name, counts, g, e, c))\n",
    "    print(f\"{name:<25} {g:>10.4f} {e:>10.4f} {c:>12.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0859ec",
   "metadata": {},
   "source": [
    "### Visualisation des fonctions d'impurete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.linspace(0.001, 0.999, 100)\n",
    "\n",
    "gini_values = [gini([p * 100, (1 - p) * 100]) for p in p_values]\n",
    "entropy_values = [entropy([p * 100, (1 - p) * 100]) for p in p_values]\n",
    "error_values = [classification_error([p * 100, (1 - p) * 100]) for p in p_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_values, gini_values, 'b-', linewidth=2, label='Gini')\n",
    "plt.plot(p_values, entropy_values, 'r-', linewidth=2, label='Entropie')\n",
    "plt.plot(p_values, error_values, 'g-', linewidth=2, label='Erreur de classification')\n",
    "\n",
    "for name, counts, g, e, c in results:\n",
    "    p = counts[0] / sum(counts)\n",
    "    plt.scatter([p], [g], color='blue', s=100, zorder=5)\n",
    "    plt.scatter([p], [e], color='red', s=100, zorder=5)\n",
    "    plt.scatter([p], [c], color='green', s=100, zorder=5)\n",
    "\n",
    "plt.xlabel('Proportion de la classe positive (p)', fontsize=12)\n",
    "plt.ylabel('Impurete', fontsize=12)\n",
    "plt.title('Comparaison des mesures d\\'impurete', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a06b4",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2 - Implementation d'un mini-arbre de decision \"from scratch\"\n",
    "\n",
    "## 2.1 Jeu de donnees pedagogique - Credit simplifie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: decision d'accorder un credit\n",
    "data = np.array([\n",
    "    [1, 1, 50, 0],  # Proprio, Matrimonial, Revenu, Defaut\n",
    "    [0, 0, 30, 1],\n",
    "    [1, 2, 45, 0],\n",
    "    [0, 1, 35, 0],\n",
    "    [1, 0, 60, 0],\n",
    "    [0, 0, 25, 1],\n",
    "    [1, 1, 55, 1],\n",
    "    [0, 2, 40, 0],\n",
    "    [1, 0, 70, 0],\n",
    "    [0, 1, 28, 1],\n",
    "    [1, 2, 48, 0],\n",
    "    [0, 0, 32, 0],\n",
    "])\n",
    "\n",
    "labels = np.array([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "feature_names = ['Proprietaire', 'Etat_Matrimonial', 'Revenu', 'Defaut_Precedent']\n",
    "class_names = ['Refuse', 'Accorde']\n",
    "\n",
    "print(\"Dataset Credit Simplifie\")\n",
    "print(f\"Nombre d'exemples: {len(data)}\")\n",
    "print(f\"Distribution des classes: {Counter(labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdbc80",
   "metadata": {},
   "source": [
    "## 2.2 Structure de noeud et fonction de meilleur split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_counts(y):\n",
    "    counts = [0, 0]\n",
    "    for label in y:\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.prediction = None\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.gini = None\n",
    "        self.samples = None\n",
    "\n",
    "def find_best_split(X, y, feature_types):\n",
    "    n_samples, n_features = X.shape\n",
    "    parent_gini = gini(get_class_counts(y))\n",
    "    \n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    \n",
    "    for feature_idx in range(n_features):\n",
    "        values = X[:, feature_idx]\n",
    "        \n",
    "        if feature_types[feature_idx] == 'continuous':\n",
    "            sorted_values = np.unique(values)\n",
    "            thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                left_mask = values <= thresh\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_gini = gini(get_class_counts(y[left_mask]))\n",
    "                right_gini = gini(get_class_counts(y[right_mask]))\n",
    "                \n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = np.sum(right_mask)\n",
    "                weighted_gini = (n_left * left_gini + n_right * right_gini) / n_samples\n",
    "                \n",
    "                gain = parent_gini - weighted_gini\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = thresh\n",
    "        else:\n",
    "            unique_values = np.unique(values)\n",
    "            for val in unique_values:\n",
    "                left_mask = values == val\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_gini = gini(get_class_counts(y[left_mask]))\n",
    "                right_gini = gini(get_class_counts(y[right_mask]))\n",
    "                \n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = np.sum(right_mask)\n",
    "                weighted_gini = (n_left * left_gini + n_right * right_gini) / n_samples\n",
    "                \n",
    "                gain = parent_gini - weighted_gini\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = val\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011cbdb0",
   "metadata": {},
   "source": [
    "## 2.3 Construction recursive de l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, feature_types, depth=0, max_depth=None, min_samples_leaf=1):\n",
    "    node = Node()\n",
    "    node.samples = len(y)\n",
    "    node.gini = gini(get_class_counts(y))\n",
    "    \n",
    "    # Condition d'arret 1: noeud pur\n",
    "    if len(np.unique(y)) == 1:\n",
    "        node.is_leaf = True\n",
    "        node.prediction = y[0]\n",
    "        return node\n",
    "    \n",
    "    # Condition d'arret 2: profondeur maximale\n",
    "    if max_depth is not None and depth >= max_depth:\n",
    "        node.is_leaf = True\n",
    "        node.prediction = Counter(y).most_common(1)[0][0]\n",
    "        return node\n",
    "    \n",
    "    # Condition d'arret 3: pas assez d'exemples\n",
    "    if len(y) < 2 * min_samples_leaf:\n",
    "        node.is_leaf = True\n",
    "        node.prediction = Counter(y).most_common(1)[0][0]\n",
    "        return node\n",
    "    \n",
    "    best_feature, best_threshold, best_gain = find_best_split(X, y, feature_types)\n",
    "    \n",
    "    if best_feature is None or best_gain <= 0:\n",
    "        node.is_leaf = True\n",
    "        node.prediction = Counter(y).most_common(1)[0][0]\n",
    "        return node\n",
    "    \n",
    "    node.feature_index = best_feature\n",
    "    node.threshold = best_threshold\n",
    "    \n",
    "    if feature_types[best_feature] == 'continuous':\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "    else:\n",
    "        left_mask = X[:, best_feature] == best_threshold\n",
    "    \n",
    "    right_mask = ~left_mask\n",
    "    \n",
    "    if np.sum(left_mask) < min_samples_leaf or np.sum(right_mask) < min_samples_leaf:\n",
    "        node.is_leaf = True\n",
    "        node.prediction = Counter(y).most_common(1)[0][0]\n",
    "        return node\n",
    "    \n",
    "    node.left = build_tree(X[left_mask], y[left_mask], feature_types, depth + 1, max_depth, min_samples_leaf)\n",
    "    node.right = build_tree(X[right_mask], y[right_mask], feature_types, depth + 1, max_depth, min_samples_leaf)\n",
    "    \n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e795f07",
   "metadata": {},
   "source": [
    "## 2.4 Fonction de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e42c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(node, x, feature_types):\n",
    "    if node.is_leaf:\n",
    "        return node.prediction\n",
    "    \n",
    "    feature_val = x[node.feature_index]\n",
    "    \n",
    "    if feature_types[node.feature_index] == 'continuous':\n",
    "        if feature_val <= node.threshold:\n",
    "            return predict_one(node.left, x, feature_types)\n",
    "        else:\n",
    "            return predict_one(node.right, x, feature_types)\n",
    "    else:\n",
    "        if feature_val == node.threshold:\n",
    "            return predict_one(node.left, x, feature_types)\n",
    "        else:\n",
    "            return predict_one(node.right, x, feature_types)\n",
    "\n",
    "def predict(node, X, feature_types):\n",
    "    return np.array([predict_one(node, x, feature_types) for x in X])\n",
    "\n",
    "def print_tree(node, feature_names, feature_types, class_names, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "    if node.is_leaf:\n",
    "        print(f\"{indent}-> Classe: {class_names[node.prediction]} (Gini: {node.gini:.3f}, n={node.samples})\")\n",
    "    else:\n",
    "        feat_name = feature_names[node.feature_index]\n",
    "        if feature_types[node.feature_index] == 'continuous':\n",
    "            print(f\"{indent}[{feat_name} <= {node.threshold:.1f}] (Gini: {node.gini:.3f}, n={node.samples})\")\n",
    "        else:\n",
    "            print(f\"{indent}[{feat_name} == {node.threshold}] (Gini: {node.gini:.3f}, n={node.samples})\")\n",
    "        print(f\"{indent}Oui:\")\n",
    "        print_tree(node.left, feature_names, feature_types, class_names, depth + 1)\n",
    "        print(f\"{indent}Non:\")\n",
    "        print_tree(node.right, feature_names, feature_types, class_names, depth + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f8ae4",
   "metadata": {},
   "source": [
    "### Entrainement et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = ['categorical', 'categorical', 'continuous', 'categorical']\n",
    "\n",
    "tree = build_tree(data, labels, feature_types, max_depth=3, min_samples_leaf=1)\n",
    "\n",
    "print(\"Structure de l'arbre:\")\n",
    "print(\"-\" * 40)\n",
    "print_tree(tree, feature_names, feature_types, class_names)\n",
    "\n",
    "predictions = predict(tree, data, feature_types)\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(f\"\\nPrecision sur l'entrainement: {accuracy * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5dba4",
   "metadata": {},
   "source": [
    "## 2.5 Comparaison avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(data, labels)\n",
    "\n",
    "sklearn_predictions = clf.predict(data)\n",
    "sklearn_accuracy = accuracy_score(labels, sklearn_predictions)\n",
    "\n",
    "print(f\"Precision mini-arbre:  {accuracy * 100:.1f}%\")\n",
    "print(f\"Precision sklearn:     {sklearn_accuracy * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nMatrice de confusion (mini-arbre):\")\n",
    "print(confusion_matrix(labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96d7e9",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 3 - Extensions : sur-apprentissage, elagage et forets aleatoires\n",
    "\n",
    "## 3.1 Selection d'un jeu de donnees reel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e57dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data = load_breast_cancer()\n",
    "X, y = bc_data.data, bc_data.target\n",
    "bc_feature_names = bc_data.feature_names\n",
    "target_names = bc_data.target_names\n",
    "\n",
    "print(f\"Dataset: Breast Cancer Wisconsin\")\n",
    "print(f\"Nombre d'exemples: {len(X)}\")\n",
    "print(f\"Nombre d'attributs: {len(bc_feature_names)}\")\n",
    "print(f\"Classes: {list(target_names)}\")\n",
    "print(f\"Distribution: {np.bincount(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c19c5",
   "metadata": {},
   "source": [
    "## 3.2 Division train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Taille train: {len(X_train)}, Taille test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1dd07",
   "metadata": {},
   "source": [
    "## 3.3 & 3.4 Effet de la profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33513180",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [1, 2, 3, 4, 5, 7, 10, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, clf.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(f\"{'max_depth':<12} {'Train Acc':>12} {'Test Acc':>12} {'Ecart':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for i, depth in enumerate(max_depths):\n",
    "    ecart = train_scores[i] - test_scores[i]\n",
    "    depth_str = str(depth) if depth else \"None\"\n",
    "    print(f\"{depth_str:<12} {train_scores[i]:>12.4f} {test_scores[i]:>12.4f} {ecart:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "x_labels = [str(d) if d else 'None' for d in max_depths]\n",
    "x_pos = range(len(max_depths))\n",
    "\n",
    "plt.plot(x_pos, train_scores, 'b-o', linewidth=2, markersize=8, label='Train')\n",
    "plt.plot(x_pos, test_scores, 'r-o', linewidth=2, markersize=8, label='Test')\n",
    "plt.xticks(x_pos, x_labels)\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effet de la profondeur sur les performances', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.85, 1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f319bf",
   "metadata": {},
   "source": [
    "### Analyse du sur-apprentissage\n",
    "\n",
    "**Observations:**\n",
    "- Quand max_depth augmente, la precision sur le train augmente (jusqu'a 100%)\n",
    "- La precision sur le test peut diminuer = **SUR-APPRENTISSAGE**\n",
    "- Un grand ecart entre train et test indique un overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e7181",
   "metadata": {},
   "source": [
    "## 3.5 Comparaison avec Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_values = [10, 50, 100, 200]\n",
    "\n",
    "print(f\"{'n_estimators':<15} {'Train Acc':>12} {'Test Acc':>12} {'F1-Score':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for n_est in n_estimators_values:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "    f1 = f1_score(y_test, rf.predict(X_test))\n",
    "    print(f\"{n_est:<15} {train_acc:>12.4f} {test_acc:>12.4f} {f1:>12.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943da71",
   "metadata": {},
   "source": [
    "## 3.6 (Optionnel) AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b832c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "best_tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "best_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"RESUME COMPARATIF\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Methode':<30} {'Test Accuracy':>15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Arbre simple (max_depth=4)':<30} {accuracy_score(y_test, best_tree.predict(X_test)):>15.4f}\")\n",
    "print(f\"{'Random Forest (n=100)':<30} {accuracy_score(y_test, best_rf.predict(X_test)):>15.4f}\")\n",
    "print(f\"{'AdaBoost (n=100)':<30} {accuracy_score(y_test, ada.predict(X_test)):>15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2efb6",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 4 - Application metier et interpretation\n",
    "\n",
    "## 4.1 Domaine d'application\n",
    "\n",
    "**DOMAINE: DIAGNOSTIC MEDICAL (Cancer du sein)**\n",
    "\n",
    "**Problematique metier:**\n",
    "Le diagnostic precoce du cancer du sein est crucial. Les medecins utilisent des biopsies \n",
    "pour analyser les caracteristiques des cellules. L'objectif est de classifier les tumeurs en:\n",
    "- BENIGNES (non cancereuses)\n",
    "- MALIGNES (cancereuses)\n",
    "\n",
    "**Variables explicatives:** 30 caracteristiques des cellules (rayon, texture, perimetre, etc.)\n",
    "\n",
    "**Variable cible:** Type de tumeur (0=Maligne, 1=Benigne)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2a162",
   "metadata": {},
   "source": [
    "## 4.2 Modele final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Performances des modeles:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Arbre de decision: {accuracy_score(y_test, tree_model.predict(X_test)):.4f}\")\n",
    "print(f\"Random Forest:     {accuracy_score(y_test, rf_model.predict(X_test)):.4f}\")\n",
    "\n",
    "print(\"\\nRapport de classification (Arbre):\")\n",
    "print(classification_report(y_test, tree_model.predict(X_test), target_names=['Maligne', 'Benigne']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f6ed3",
   "metadata": {},
   "source": [
    "## 4.3 Regles de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regles extraites de l'arbre:\")\n",
    "print(\"-\" * 50)\n",
    "print(export_text(tree_model, feature_names=list(bc_feature_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24230d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_model, feature_names=bc_feature_names, class_names=['Maligne', 'Benigne'],\n",
    "          filled=True, rounded=True, fontsize=8)\n",
    "plt.title(\"Arbre de decision pour le diagnostic du cancer du sein\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11028d8e",
   "metadata": {},
   "source": [
    "### Importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Top 10 variables les plus importantes:\")\n",
    "for i in range(10):\n",
    "    print(f\"  {i+1}. {bc_feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067cbcc",
   "metadata": {},
   "source": [
    "## 4.4 Discussion\n",
    "\n",
    "### Interpretabilite (Arbres vs Forets)\n",
    "\n",
    "| Critere | Arbre de decision | Random Forest |\n",
    "|---------|-------------------|---------------|\n",
    "| Interpretabilite | Tres bonne | Faible |\n",
    "| Performance | Moyenne | Elevee |\n",
    "| Sur-apprentissage | Risque eleve | Risque faible |\n",
    "| Visualisation | Facile | Difficile |\n",
    "\n",
    "### Limites observees\n",
    "1. **Zones d'erreur**: Cas limites difficiles a classer\n",
    "2. **Biais potentiels**: Dataset desequilibre\n",
    "3. **Donnees manquantes**: Non gerees nativement\n",
    "4. **Generalisation**: Depend de la population d'entrainement\n",
    "\n",
    "### Recommandations\n",
    "- Utiliser l'arbre comme outil d'aide, pas de remplacement du medecin\n",
    "- Confirmer avec l'expertise medicale\n",
    "- Actualiser regulierement avec de nouvelles donnees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, tree_model.predict(X_test))\n",
    "print(\"Matrice de confusion finale\")\n",
    "print(\"=\" * 40)\n",
    "print(\"              Predit\")\n",
    "print(\"           Maligne  Benigne\")\n",
    "print(f\"Reel Maligne   {cm[0,0]:3d}      {cm[0,1]:3d}\")\n",
    "print(f\"     Benigne   {cm[1,0]:3d}      {cm[1,1]:3d}\")\n",
    "print(f\"\\nAttention: {cm[0,1]} faux positifs (malignes classees benignes) = DANGEREUX\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ef4e4",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "Ce projet a permis de:\n",
    "1. Comprendre les mesures d'impurete (Gini, entropie, erreur)\n",
    "2. Implementer un arbre de decision from scratch\n",
    "3. Analyser le sur-apprentissage et les hyperparametres\n",
    "4. Comparer arbres simples, Random Forest et AdaBoost\n",
    "5. Appliquer ces modeles a un cas reel de diagnostic medical\n",
    "6. Interpreter les regles pour un non-expert\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
